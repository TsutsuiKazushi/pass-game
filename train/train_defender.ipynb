{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from common.network import DuelingNetwork\n",
    "from common.replay import PrioritizedReplayBuffer\n",
    "from common.trainer import Trainer\n",
    "from common.hparameter import *\n",
    "\n",
    "\"\"\" seed \"\"\"\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "''' divice '''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "Episode: 100,  Step: 19271, Episode_step: 157, Reward_d: 1\n",
      "Episode: 200,  Step: 29027, Episode_step: 219, Reward_d: 0\n",
      "Episode: 300,  Step: 33789, Episode_step: 21, Reward_d: 0\n",
      "Episode: 400,  Step: 43423, Episode_step: 70, Reward_d: 0\n",
      "Episode: 500,  Step: 50553, Episode_step: 101, Reward_d: 0\n",
      "Episode: 600,  Step: 60966, Episode_step: 21, Reward_d: 1\n",
      "Episode: 700,  Step: 69276, Episode_step: 24, Reward_d: 1\n",
      "Episode: 800,  Step: 75937, Episode_step: 19, Reward_d: 1\n",
      "Episode: 900,  Step: 82789, Episode_step: 61, Reward_d: 1\n",
      "Episode: 1000,  Step: 89657, Episode_step: 11, Reward_d: 1\n",
      "Finish! 0.03 [h]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Network \"\"\"\n",
    "net_d = DuelingNetwork(28, 13).to(device)\n",
    "target_net_d = DuelingNetwork(28, 13).to(device)\n",
    "optimizer_d = optim.Adam(net_d.parameters(), lr=learning_rate)\n",
    "loss_func = nn.SmoothL1Loss(reduction='none')\n",
    "net_a = DuelingNetwork(20, 3).to(device) #dummy\n",
    "\n",
    "\"\"\" Replay buffer \"\"\"\n",
    "replay_buffer_d = PrioritizedReplayBuffer(buffer_size)\n",
    "beta_func = lambda step: min(beta_end, beta_begin + (beta_end - beta_begin) * (step / beta_decay))\n",
    "\n",
    "\"\"\" Epsilon \"\"\"\n",
    "epsilon_func = lambda step: max(epsilon_end, epsilon_begin - (epsilon_begin - epsilon_end) * (step / epsilon_decay))\n",
    "\n",
    "\"\"\" Trainer \"\"\"\n",
    "trainer_d = Trainer(net_d, target_net_d, optimizer_d, loss_func, replay_buffer_d, gamma, device)\n",
    "\n",
    "\"\"\" Environment \"\"\"\n",
    "from three_on_one import ThreeOnOne\n",
    "accel_d = 2\n",
    "accel_a = 2\n",
    "env = ThreeOnOne(accel_defender=accel_d, accel_attacker1=accel_a, accel_attacker2=accel_a, accel_attacker3=accel_a, max_step=max_step_episode)\n",
    "\n",
    "filename_model_d = \"../model/defender_test.pth\"\n",
    "\n",
    "\"\"\"for test\"\"\"\n",
    "num_episodes = 1000\n",
    "print_interval_episode = 100\n",
    "save_interval_episode = 100\n",
    "initial_buffer_size = 1000\n",
    "\n",
    "print('Start!')\n",
    "start_time = time.time()\n",
    "\n",
    "step = 0\n",
    "for episode in range(num_episodes):\n",
    "    step_episode = 0\n",
    "    obs_d, obs_a1, obs_a2, obs_a3, with_b_a1, with_b_a2, with_b_a3 = env.reset()\n",
    "    obs_d, obs_a1, obs_a2, obs_a3 = torch.Tensor(obs_d), torch.Tensor(obs_a1), torch.Tensor(obs_a2), torch.Tensor(obs_a3)\n",
    "    done = False\n",
    "    total_reward_d = 0\n",
    "    total_reward_a1 = 0\n",
    "    total_reward_a2 = 0\n",
    "    total_reward_a3 = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action_d = net_d.act(obs_d.float().to(device), epsilon_func(step))\n",
    "        \n",
    "        action_a1 = net_a.act(obs_a1.float().to(device), epsilon_func(step)) #dummy\n",
    "        action_a2 = net_a.act(obs_a2.float().to(device), epsilon_func(step)) #dummy\n",
    "        action_a3 = net_a.act(obs_a3.float().to(device), epsilon_func(step)) #dummy\n",
    "                \n",
    "        with_b_a1_tmp, with_b_a2_tmp, with_b_a3_tmp = with_b_a1, with_b_a2, with_b_a3\n",
    "        \n",
    "        next_obs_d, next_obs_a1, next_obs_a2, next_obs_a3, reward_d, reward_a1, reward_a2, reward_a3, done, obs_a, action_a, reward_a, next_obs_a, push_a, with_b_a1, with_b_a2, with_b_a3, to_a1, to_a2, to_a3 = \\\n",
    "            env.step(obs_d, obs_a1, obs_a2, obs_a3, action_d, action_a1, action_a2, action_a3, step_episode)\n",
    "        \n",
    "        next_obs_d, next_obs_a1, next_obs_a2, next_obs_a3 = torch.Tensor(next_obs_d), torch.Tensor(next_obs_a1), torch.Tensor(next_obs_a2), torch.Tensor(next_obs_a3)\n",
    "                \n",
    "        total_reward_d += reward_d\n",
    "        total_reward_a1 += reward_a1\n",
    "        total_reward_a2 += reward_a2\n",
    "        total_reward_a3 += reward_a3\n",
    "        \n",
    "        replay_buffer_d.push([obs_d, action_d, reward_d, next_obs_d, done])\n",
    "                \n",
    "        obs_d = next_obs_d\n",
    "        obs_a1 = next_obs_a1\n",
    "        obs_a2 = next_obs_a2\n",
    "        obs_a3 = next_obs_a3\n",
    "\n",
    "        if len(replay_buffer_d) >= initial_buffer_size:\n",
    "            trainer_d.update(batch_size, beta_func(step))\n",
    "            \n",
    "        if (step + 1) % target_update_interval == 0:\n",
    "            target_net_d.load_state_dict(net_d.state_dict())\n",
    "        \n",
    "        step += 1\n",
    "        step_episode += 1\n",
    "\n",
    "    if (episode + 1) % print_interval_episode == 0:\n",
    "        print(f'Episode: {episode + 1},  Step: {step + 1}, Episode_step: {step_episode + 1}, Reward_d: {round(total_reward_d, 2)}')\n",
    "\n",
    "#     if (episode + 1) % save_interval_episode == 0:\n",
    "#         torch.save(net_d.state_dict(), filename_model_d)\n",
    "        \n",
    "end_time = time.time()\n",
    "diff_time = (end_time - start_time)/3600\n",
    "print('Finish!', round(diff_time, 2), '[h]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
